{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dedependiecies \n",
    "\n",
    "# Web scrape\n",
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import psycopg2\n",
    "\n",
    "# SQL\n",
    "from sqlalchemy import create_engine, inspect\n",
    "from sqlalchemy.ext.automap import automap_base\n",
    "from sqlalchemy.orm import Session\n",
    "\n",
    "# General\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open browser with splinter \n",
    "# (Headless = True) Thus we do not see the browser oepning and closing\n",
    "executable_path = {'executable_path': 'chromedriver.exe'}\n",
    "browser = Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTML Object and Beautiful Soup Parseing\n",
    "url = 'https://www.nei.org/resources/statistics/state-electricity-generation-fuel-shares'\n",
    "browser.visit(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTML Object and Beautiful Soup Parseing\n",
    "html = browser.html\n",
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of scraped data: 510\n",
      "0.2\n"
     ]
    }
   ],
   "source": [
    "# Retrieve and collect all desired data\n",
    "webTable = soup.find_all('table')\n",
    "\n",
    "# All desired data is in one line, thus have to split\n",
    "us_percent_list = []\n",
    "temp_list = []\n",
    "\n",
    "for result in webTable:\n",
    "    tableData = []\n",
    "    us_percent_list.append([])\n",
    "    tableData = result.find_all('td')\n",
    "    \n",
    "#print(us_percent_list) \n",
    "print(f'Length of scraped data: {len(tableData)}')\n",
    "print(tableData[509].text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['Alabama'], ['31.4'], ['19.2'], ['38.7'], ['0.0'], ['7.8'], ['0.0'], ['0.3'], ['0.0'], ['2.5'], ['Alaska'], ['0.0'], ['10.6'], ['49.9'], ['14.4'], ['22.0'], ['0.0'], ['0.0'], ['2.4'], ['0.6'], ['Arizona'], ['28.1'], ['20.4'], ['40.9'], ['0.1'], ['5.4'], ['0.0'], ['4.5'], ['0.5'], ['0.2'], ['Arkansas'], ['22.0'], ['37.9'], ['33.0'], ['0.1'], ['4.5'], ['0.0'], ['0.3'], ['0.0'], ['2.2'], ['California'], ['7.9'], ['0.1'], ['42.4'], ['0.0'], ['19.5'], ['5.6'], ['14.0'], ['7.3'], ['3.2'], ['Colorado'], ['0.0'], ['45.1'], ['30.4'], ['0.0'], ['2.5'], ['0.0'], ['2.1'], ['19.4'], ['0.4'], ['Connecticut'], ['41.7'], ['0.1'], ['52.8'], ['0.2'], ['1.3'], ['0.0'], ['0.4'], ['0.0'], ['3.3'], ['Delaware'], ['0.0'], ['2.3'], ['94.7'], ['0.8'], ['0.0'], ['0.0'], ['1.0'], ['0.1'], ['1.0'], ['District of Columbia'], ['0.0'], ['0.0'], ['27.1'], ['0.0'], ['0.0'], ['0.0'], ['15.1'], ['0.0'], ['57.8'], ['Florida'], ['11.8'], ['9.1'], ['74.3'], ['0.1'], ['0.1'], ['0.0'], ['1.6'], ['0.0'], ['3.1'], ['Georgia'], ['26.3'], ['19.9'], ['45.7'], ['0.1'], ['2.3'], ['0.0'], ['1.6'], ['0.0'], ['4.1'], ['Hawaii'], ['0.0'], ['13.2'], ['0.1'], ['69.3'], ['0.7'], ['1.1'], ['2.7'], ['6.0'], ['6.9'], ['Idaho'], ['0.0'], ['0.1'], ['21.1'], ['0.0'], ['55.4'], ['0.4'], ['3.3'], ['16.2'], ['3.4'], ['Illinois'], ['54.4'], ['26.9'], ['10.6'], ['0.0'], ['0.1'], ['0.0'], ['0.0'], ['7.6'], ['0.4'], ['Indiana'], ['0.0'], ['56.7'], ['35.9'], ['0.1'], ['0.2'], ['0.0'], ['0.3'], ['6.0'], ['0.7'], ['Iowa1'], ['8.3'], ['35.2'], ['12.9'], ['0.1'], ['1.2'], ['0.0'], ['0.0'], ['42.0'], ['0.3'], ['Kansas'], ['17.8'], ['33.4'], ['7.0'], ['0.1'], ['0.0'], ['0.0'], ['0.0'], ['41.4'], ['0.1'], ['Kentucky'], ['0.0'], ['72.7'], ['20.8'], ['0.1'], ['5.6'], ['0.0'], ['0.1'], ['0.0'], ['0.7'], ['Louisiana'], ['13.8'], ['10.4'], ['72.0'], ['0.0'], ['1.1'], ['0.0'], ['0.0'], ['0.0'], ['2.6'], ['Maine'], ['0.0'], ['0.8'], ['15.9'], ['0.3'], ['30.8'], ['0.0'], ['0.1'], ['23.8'], ['28.3'], ['Maryland'], ['37.8'], ['14.4'], ['37.4'], ['0.2'], ['5.6'], ['0.0'], ['1.3'], ['1.5'], ['1.8'], ['Massachusetts'], ['9.8'], ['0.0'], ['71.6'], ['0.6'], ['2.9'], ['0.0'], ['5.3'], ['1.0'], ['8.8'], ['Michigan'], ['28.2'], ['32.7'], ['31.1'], ['0.1'], ['0.5'], ['0.0'], ['0.1'], ['5.0'], ['2.2'], ['Minnesota'], ['24.4'], ['31.5'], ['17.9'], ['0.1'], ['1.5'], ['0.1'], ['2.6'], ['19.1'], ['2.9'], ['Mississippi'], ['16.5'], ['6.6'], ['74.3'], ['0.0'], ['0.0'], ['0.0'], ['0.5'], ['0.0'], ['2.1'], ['Missouri'], ['12.0'], ['72.8'], ['9.6'], ['0.1'], ['1.4'], ['0.0'], ['0.1'], ['3.8'], ['0.1'], ['Montana'], ['0.0'], ['53.9'], ['2.0'], ['0.1'], ['34.7'], ['0.6'], ['0.1'], ['8.6'], ['0.1'], ['Nebraska'], ['18.7'], ['54.7'], ['3.3'], ['0.1'], ['3.1'], ['0.0'], ['0.1'], ['19.9'], ['0.2'], ['Nevada'], ['0.0'], ['6.9'], ['64.8'], ['0.0'], ['5.6'], ['9.5'], ['12.2'], ['0.8'], ['0.2'], ['New Hampshire'], ['60.8'], ['1.9'], ['20.0'], ['0.2'], ['6.9'], ['0.0'], ['0.0'], ['2.3'], ['8.1'], ['New Jersey'], ['37.4'], ['1.6'], ['57.3'], ['0.1'], ['(0.1)'], ['0.0'], ['1.7'], ['0.0'], ['2.0'], ['New Mexico'], ['0.0'], ['41.8'], ['34.1'], ['0.1'], ['0.4'], ['0.2'], ['3.9'], ['19.5'], ['0.0'], ['New York'], ['33.9'], ['0.3'], ['37.3'], ['0.4'], ['22.1'], ['0.0'], ['0.5'], ['3.3'], ['2.2'], ['North Carolina'], ['31.8'], ['23.3'], ['31.7'], ['0.2'], ['4.7'], ['0.0'], ['5.5'], ['0.4'], ['2.3'], ['North Dakota'], ['0.0'], ['61.9'], ['3.2'], ['0.1'], ['6.9'], ['0.3'], ['0.0'], ['27.5'], ['0.1'], ['Ohio'], ['14.2'], ['39.8'], ['43.2'], ['0.1'], ['0.2'], ['0.0'], ['0.1'], ['1.7'], ['0.6'], ['Oklahoma'], ['0.0'], ['9.4'], ['53.5'], ['0.0'], ['2.1'], ['0.0'], ['0.1'], ['34.6'], ['0.4'], ['Oregon'], ['0.0'], ['4.1'], ['33.9'], ['0.0'], ['47.4'], ['0.2'], ['1.1'], ['11.5'], ['1.8'], ['Pennsylvania'], ['36.0'], ['16.8'], ['42.8'], ['0.1'], ['1.5'], ['0.0'], ['0.0'], ['1.5'], ['1.3'], ['Rhode Island'], ['0.0'], ['0.0'], ['93.2'], ['0.2'], ['0.0'], ['0.0'], ['0.9'], ['2.9'], ['2.7'], ['South Carolina'], ['55.8'], ['14.8'], ['24.1'], ['0.1'], ['1.9'], ['0.0'], ['0.8'], ['0.0'], ['2.4'], ['South Dakota'], ['0.0'], ['20.7'], ['10.7'], ['0.1'], ['44.6'], ['0.0'], ['0.0'], ['23.9'], ['0.0'], ['Tennessee'], ['43.7'], ['22.9'], ['20.3'], ['0.2'], ['11.3'], ['0.0'], ['0.4'], ['0.0'], ['1.1'], ['Texas'], ['8.6'], ['19.1'], ['53.4'], ['0.0'], ['0.2'], ['0.0'], ['0.9'], ['17.5'], ['0.4'], ['Utah'], ['0.0'], ['64.4'], ['24.1'], ['0.1'], ['2.0'], ['1.1'], ['5.5'], ['2.1'], ['0.7'], ['Vermont'], ['0.0'], ['0.0'], ['0.1'], ['0.1'], ['54.9'], ['0.0'], ['8.0'], ['17.6'], ['19.4'], ['Virginia'], ['30.3'], ['3.6'], ['60.1'], ['0.3'], ['0.5'], ['0.0'], ['0.9'], ['0.0'], ['4.5'], ['Washington'], ['8.4'], ['6.8'], ['13.3'], ['0.0'], ['62.6'], ['0.0'], ['0.0'], ['7.3'], ['1.6'], ['West Virginia'], ['0.0'], ['91.0'], ['3.4'], ['0.2'], ['2.8'], ['0.0'], ['0.0'], ['2.7'], ['(0.0)'], ['Wisconsin'], ['16.0'], ['42.1'], ['33.7'], ['0.1'], ['3.2'], ['0.0'], ['0.1'], ['2.6'], ['2.2'], ['Wyoming'], ['0.0'], ['84.3'], ['3.2'], ['0.1'], ['2.0'], ['0.0'], ['0.4'], ['9.8'], ['0.2']]]\n"
     ]
    }
   ],
   "source": [
    "# Retrieve and collect all desired data\n",
    "webTableRows = soup.find_all('tr')\n",
    "\n",
    "# All desired data is in one line, thus have to split\n",
    "us_percent_list = []\n",
    "temp_list_small = []\n",
    "temp_list_big = []\n",
    "\n",
    "for tr in webTableRows:\n",
    "    tableData = tr.find_all('td')\n",
    "    tableColumnHeader = tr.find_all('th')\n",
    "    us_percent_list= []\n",
    "\n",
    "    for td in tableData:\n",
    "        data = td.text\n",
    "        temp_list_small.append(data)\n",
    "        temp_list_big.append(temp_list_small)\n",
    "        temp_list_small = []\n",
    "\n",
    "    us_percent_list.append(temp_list_big)\n",
    "print(us_percent_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rows = soup.find(\"table\").find(\"tbody\").find_all(\"tr\")\n",
    "#, border=1\n",
    "for row in rows:\n",
    "    cells = row.find_all(\"td\")\n",
    "    rn = cells.text()\n",
    "print (rn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data into df and clean\n",
    "#US_percentage_df = pd.DataFrame(us_percent_list, columns = tableHeaders)\n",
    "US_percentage_df = pd.DataFrame(us_percent_list, columns = [\"State\", \"Nuclear (%)\", \"Coal (%)\", \"Natural Gas (%)\", \n",
    "                                                            \"Petroleum (%)\", \"Hydro (%)\", \"Geothermal (%)\", \n",
    "                                                            \"Solar - PV (%)\", \"Wind (%)\", \"Biomass and Other (%)\"])\n",
    "\n",
    "\n",
    "print(f'The length of this df is: {len(US_percentage_df)}')\n",
    "US_percentage_df.head()\n",
    "\n",
    "#### ValueError: 10 columns passed, passed data had 510 columns  ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Possible needed code for web scraping #######\n",
    "# Convert years to integers \n",
    "# insider_df['Year'] = (insider_df['Year']).astype(int)\n",
    "\n",
    "# Removing extra years \n",
    "#insider_df = insider_df[insider_df['Year'] <= 2019]\n",
    "#insider_df = insider_df[insider_df['Year'] >= 2010]\n",
    "#insider_df = insider_df.sort_values(by=['Year'])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dont forget to quit it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finished scraping from the web, quit browser in background\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV Load "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Source 1\n",
    "# read from local file\n",
    "raw_csv_1 = \"./Resources/name1.csv\"\n",
    "CSV_df_1 = pd.read_csv(raw_csv_1, sep = ',', encoding='ISO-8859-1')\n",
    "\n",
    "# Display head of data set to be sure it was loaded correctly\n",
    "CSV_df_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Source 2\n",
    "# read from local file\n",
    "raw_csv_2 = \"./Resources/name2.csv\"\n",
    "CSV_df_2 = pd.read_csv(raw_csv_1, sep = ',', encoding='ISO-8859-1')\n",
    "\n",
    "# Display head of data set to be sure it was loaded correctly\n",
    "CSV_df_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Source 3\n",
    "# read from local file\n",
    "raw_csv_3 = \"./Resources/name3.csv\"\n",
    "CSV_df_3 = pd.read_csv(raw_csv_1, sep = ',', encoding='ISO-8859-1')\n",
    "\n",
    "# Display head of data set to be sure it was loaded correctly\n",
    "CSV_df_3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add CSV and json to SQL, Postgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to local database\n",
    "code_source_passcode = open('/Users/Richa/Desktop/Files/SQL_private_connect.py') \n",
    "# Add you password here\n",
    "#code_source_passcode = open('/Users/......./SQL_private_connect.py') \n",
    "\n",
    "sql_private_connect = code_source_passcode.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(f'postgresql://{sql_private_connect}@localhost:5432/Need_To_Add_(name)_db')\n",
    "connection = engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['combined_music', 'insider', 'billboard_top', 'spotify']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for tables\n",
    "engine.table_names() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Web_Scraped_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-cefa14b08b55>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Use pandas to load json converted DataFrame into database\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m Web_Scraped_df.to_sql(name='us_percentage', \n\u001b[0m\u001b[0;32m      3\u001b[0m                       \u001b[0mcon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                       \u001b[0mif_exists\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'replace'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                       index=False)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Web_Scraped_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Use pandas to load json converted DataFrame into database\n",
    "Web_Scraped_df.to_sql(name='us_percentage', \n",
    "                      con=engine, \n",
    "                      if_exists='replace', \n",
    "                      index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pandas to load csv converted DataFrame into database\n",
    "CSV_df_1.to_sql(name='name1', \n",
    "               con=engine, \n",
    "               if_exists='replace', \n",
    "               index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pandas to load csv converted DataFrame into database\n",
    "CSV_df_2.to_sql(name='name2', \n",
    "               con=engine, \n",
    "               if_exists='replace', \n",
    "               index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pandas to load csv converted DataFrame into database\n",
    "CSV_df_3.to_sql(name='name3', \n",
    "               con=engine, \n",
    "               if_exists='replace', \n",
    "               index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for tables\n",
    "engine.table_names() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check tables and data before making charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm data has been added by querying the insider table\n",
    "SQL_CSV1_loaded = pd.read_sql_query('select * from name1', con=engine)\n",
    "SQL_CSV1_loaded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm data has been added by querying the insider table\n",
    "SQL_CSV1_loaded = pd.read_sql_query('select * from name1', con=engine)\n",
    "SQL_CSV1_loaded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm data has been added by querying the insider table\n",
    "SQL_CSV1_loaded = pd.read_sql_query('select * from name1', con=engine)\n",
    "SQL_CSV1_loaded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm data has been added by querying the insider table\n",
    "SQL_CSV1_loaded = pd.read_sql_query('select * from name1', con=engine)\n",
    "SQL_CSV1_loaded.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:PythonData] *",
   "language": "python",
   "name": "conda-env-PythonData-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
